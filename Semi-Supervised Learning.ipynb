{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks.\n",
    "This module includes Label Propagation.\n",
    "\n",
    "Semi-supervised learning is a situation in which in your training data some of the samples are not labeled. \n",
    "The semi-supervised estimators in sklearn.semi_supervised are able to make use of this additional unlabeled data to \n",
    "better capture the shape of the underlying data distribution and generalize better to new samples. \n",
    "These algorithms can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "df=pd.read_excel('H:/lottery/ssq.xls',sheet_name='data')\n",
    "df.columns=['num', 'date', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'b', 'sr1', 'sr2',\n",
    "       'sr3', 'sr4', 'sr5', 'sr6', 'sales_volume', 'pool', 'first', 'bonus1', 'second', 'bonus2',\n",
    "       'third', 'bonus3', 'fourth', 'bonus4', 'fifth', 'bonus5', 'sexth', 'bonus6']\n",
    "#df.tail()\n",
    "X = df[[ 'b']]\n",
    "X.columns=['sr0']\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "num_rows = X.shape[0]\n",
    "def creat_datasets(X, features ):\n",
    "    df_ = pd.DataFrame(columns = list(range(0, features)))\n",
    "    index = 0\n",
    "    for i in range(features, num_rows+1):\n",
    "        df_.loc[index] = X[col][i-features: i].reset_index(drop = True)\n",
    "        index += 1\n",
    "    return df_\n",
    "\n",
    "for col in X.columns :\n",
    "    locals()['df_'+ str(col)] = creat_datasets(X, 200)\n",
    "endtime = datetime.now()\n",
    "Time = endtime - starttime\n",
    "print(Time.seconds, 'seconds')\n",
    "# 至此据集完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_c(df_):\n",
    "    # choose frequence >10 samples\n",
    "    #df = df_[df_[99].isin(df_[99].groupby(df_[99]).count()[df_[99].groupby(df_[99]).count()>30].index)]\n",
    "    X_ = df_.iloc[:, : -1].values\n",
    "    y_ = df_.iloc[:, -1].astype(int).values #dataframe.as_matrix() \n",
    "    ''' why need astpye(int/str), otherwise error with y_type=object??'''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=14)\n",
    "    return X_, y_, X_train, X_test, y_train, y_test\n",
    "def score_c(clf, cv=3):\n",
    "    precision = cross_val_score(clf, X_, y_, cv=cv, scoring='precision_weighted')\n",
    "    recall = cross_val_score(clf, X_, y_, cv=cv, scoring='recall_weighted')\n",
    "    fls = cross_val_score(clf, X_, y_, cv=cv, scoring='f1_weighted')\n",
    "    return precision, recall, fls\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    '''if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)'''\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def total_classifier(pipe_LinearSVC, param_LinearSVC):\n",
    "    X_ = df_.iloc[:, : -1].values\n",
    "    y_ = df_.iloc[:, -1].astype(int).values.reshape(-1)\n",
    "    #dataframe.as_matrix()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=14)\n",
    "    \n",
    "    grid_=GridSearchCV(pipe_LinearSVC, param_LinearSVC, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "    #default=None, and the default scorer of DT is accuracy. see also DecisionTreeClassifier.score(X, y)\n",
    "    grid_.fit(X_train, y_train)\n",
    "    # evaluation for best parameters\n",
    "    precision = cross_val_score(grid_.best_estimator_, X_, y_, cv=3, scoring='precision_weighted')\n",
    "    recall = cross_val_score(grid_.best_estimator_, X_, y_, cv=3, scoring='recall_weighted')\n",
    "    fls = cross_val_score(grid_.best_estimator_, X_, y_, cv=3, scoring='f1_weighted')\n",
    "    new = grid_.predict(df_.iloc[-1,1: ].values.reshape(1,-1))\n",
    "    np.set_printoptions(precision=4)\n",
    "    print('df_R'+ str(i), new, 'precison:', precision, 'recall:', recall, 'f1:', fls)\n",
    "    print(grid_.best_estimator_)\n",
    "    \n",
    "    class_names = np.unique(y_train)\n",
    "    y_pred = grid_.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    c = np.sum(y_test == y_pred)\n",
    "    print(report)\n",
    "    print('n_accuracy:', c)\n",
    "    #print(grid_.scorer_, grid_.best_score_, grid_.cv_results_)\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM \n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "pipe_BernoulliRBM = Pipeline([('clf', BernoulliRBM(random_state14))])\n",
    "param_BernoulliRBM = {'clf__n_components': [256, ], #default = 256; Number of binary hidden units\n",
    "                       'clf__learning_rate': [0.1, ], #default =0.1 .Reasonable values are in the 10**[0., -3.] range.\n",
    "                      #The learning rate for weight updates. It is highly recommended to tune this hyper-parameter. \n",
    "                       'clf__batch_size': [10, ], #default=10; Number of examples per minibatch.\n",
    "                       'clf__n_iter': [10, ] , #default=30; Leaf size passed to BallTree or KDTree. \n",
    "                       #This can affect the speed of the construction and query, as well as the memory required to store the tree. \n",
    "                       #The optimal value depends on the nature of the problem.\n",
    "                       'clf__verbose': [0, ], #default=2; Power parameter for the Minkowski metric.\n",
    "                      }\n",
    "for i in range(0,1):\n",
    "    df_ =  locals()['df_sr'+ str(i)]\n",
    "    X_ = fit_transform(X_)\n",
    "\n",
    "endtime = datetime.now()\n",
    "Time = endtime - starttime\n",
    "print(Time.seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
